{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='lesson13_data.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2ascii_arr(name):\n",
    "    \"\"\"\n",
    "    0-255\n",
    "    \"\"\"\n",
    "    arr = [ord(c) for c in name]\n",
    "    return arr, len(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size=256, hidden_size=256, output_size=18, n_layers=1):\n",
    "        \"\"\"\n",
    "        Because word embedding is working with ascii. It has to use `input_size=256, hidden_size=256`\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # input_size 256, hidden_size 256.\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Sung Kim run this all at once (over the whole input sequence)\n",
    "        # input = B x S . size(0) = B\n",
    "        batch_size = input.size(0)\n",
    "        \n",
    "        # input: B x S -- (transpose) --> S x B\n",
    "        input = input.t()\n",
    "        \n",
    "        # Embedding S x B -> S x B x I (embedding size)\n",
    "        print(f\" input size: {input.size()}\")\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = embedded.clone().detach() # Make new tensor because of `EmbeddingGrad`\n",
    "        print(f\" embeddding size: {embedded.size()}\")\n",
    "        \n",
    "        # Make a hidden\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        print(f\" gru hidden output: {hidden.size()}\")\n",
    "        \n",
    "        # Use last layer output as FC's input\n",
    "        # No need to unpack, since we are going to use hidden\n",
    "        fc_output = self.fc(hidden)\n",
    "        print(f\" fc output: {fc_output.size()}\")\n",
    "        return fc_output\n",
    "        \n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        return torch.tensor(hidden, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in torch.Size([1, 6]) 'adylov'\n",
    "# out torch.Size([1, 1, 18]) 18 countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='zero_padding.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(vectorized_seqs, seq_lengths):\n",
    "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max()), dtype=torch.long)\n",
    "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        seq_tensor[idx, :seq_len] = torch.tensor(seq, dtype=torch.long)\n",
    "    return seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_variables(names):\n",
    "    sequence_and_length = [str2ascii_arr(name) for name in names]\n",
    "    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n",
    "    seq_lengths = torch.tensor([sl[1] for sl in sequence_and_length])\n",
    "    return pad_sequences(vectorized_seqs, seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 97, 122],\n",
       "        [ 97,  98]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_variables(['az', 'ab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input size: torch.Size([6, 1])\n",
      " embeddding size: torch.Size([6, 1, 256])\n",
      " gru hidden output: torch.Size([1, 1, 256])\n",
      " fc output: torch.Size([1, 1, 18])\n",
      "\n",
      "in: torch.Size([1, 6]), \n",
      "out: torch.Size([1, 1, 18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "classifier = RNNClassifier()\n",
    "arr, _ = str2ascii_arr('adylov')\n",
    "inp = torch.tensor([arr], dtype=torch.long)\n",
    "out = classifier(inp)\n",
    "print(f\"\\nin: {inp.size()}, \\nout: {out.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input size: torch.Size([6, 4])\n",
      " embeddding size: torch.Size([6, 4, 256])\n",
      " gru hidden output: torch.Size([1, 4, 256])\n",
      " fc output: torch.Size([1, 4, 18])\n",
      "\n",
      "batch in: torch.Size([4, 6]), \n",
      "batch out: torch.Size([1, 4, 18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "names = ['adylov', 'solan', 'hard', 'san']\n",
    "classifier = RNNClassifier()\n",
    "inputs = make_variables(names)\n",
    "out = classifier(inputs)\n",
    "print(f\"\\nbatch in: {inputs.size()}, \\nbatch out: {out.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = pd.read_csv('names_train.csv', header=None) # 2 * 9 * 743\n",
    "testset = pd.read_csv('names_test.csv', header=None) # 4 * 25 * 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['name', 'country']\n",
    "trainset.columns = headers\n",
    "testset.columns = headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Czech',\n",
       " 'German',\n",
       " 'Arabic',\n",
       " 'Japanese',\n",
       " 'Chinese',\n",
       " 'Vietnamese',\n",
       " 'Russian',\n",
       " 'French',\n",
       " 'Irish',\n",
       " 'English',\n",
       " 'Spanish',\n",
       " 'Greek',\n",
       " 'Italian',\n",
       " 'Portuguese',\n",
       " 'Scottish',\n",
       " 'Dutch',\n",
       " 'Korean',\n",
       " 'Polish']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries = list(trainset.country.drop_duplicates())\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Russian       6272\n",
       "English       2445\n",
       "Arabic        1333\n",
       "Japanese       660\n",
       "German         482\n",
       "Italian        472\n",
       "Czech          346\n",
       "Spanish        198\n",
       "Dutch          198\n",
       "French         184\n",
       "Chinese        178\n",
       "Irish          154\n",
       "Greek          135\n",
       "Polish          92\n",
       "Scottish        66\n",
       "Korean          62\n",
       "Portuguese      49\n",
       "Vietnamese      48\n",
       "Name: country, dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Majority of dataset is `Russian`\n",
    "trainset.country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Russian       3136\n",
       "English       1223\n",
       "Arabic         667\n",
       "Japanese       331\n",
       "German         242\n",
       "Italian        237\n",
       "Czech          173\n",
       "Spanish        100\n",
       "Dutch           99\n",
       "French          93\n",
       "Chinese         90\n",
       "Irish           78\n",
       "Greek           68\n",
       "Polish          47\n",
       "Scottish        34\n",
       "Korean          32\n",
       "Vietnamese      25\n",
       "Portuguese      25\n",
       "Name: country, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So as trainset\n",
    "testset.country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Czech'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.iloc[0]['country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataSet(Dataset):\n",
    "    def __init__(self, filename='names_train.csv'):\n",
    "        trainset = pd.read_csv('names_train.csv', header=None)\n",
    "        trainset.columns = ['name', 'country']\n",
    "        countries = list(trainset.country.drop_duplicates())\n",
    "\n",
    "        self.trainset = trainset\n",
    "        self.countries = countries\n",
    "        self.len = len(trainset)        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        country = self.trainset.iloc[index]['country']\n",
    "        return self.trainset.iloc[index]['name'], self.countries.index(country)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NameDataSet()\n",
    "test_dataset = NameDataSet('names_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.countries.index('Czech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=2, num_workers=2, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=4, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
