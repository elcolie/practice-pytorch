{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancement\n",
    "1. Dataset uses `pandas` not `numpy`\n",
    "1. Data normalization\n",
    "1. Drop out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabeteDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        raw_df = pd.read_csv(csv_file)\n",
    "        df = (raw_df - raw_df.mean())/raw_df.std()\n",
    "        \n",
    "        self.len = df.shape[0]\n",
    "\n",
    "        x_columns = list(df.columns)\n",
    "        x_columns.remove('Class')\n",
    "        x_columns\n",
    "        \n",
    "        self.x_data = df[x_columns]\n",
    "        self.y_data = raw_df['Class']\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.x_data.iloc[index], dtype=torch.float32), bool(self.y_data.iloc[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI\n",
    "https://github.com/MateLabs/Public-Datasets/blob/master/Datasets/diabetes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sampler import ImbalancedDatasetSampler\n",
    "\n",
    "# train_dataset = DiabeteDataset('diabetes_train.csv') # 768 rows\n",
    "\n",
    "train_dataset = DiabeteDataset('diabetes_train.csv')\n",
    "test_dataset = DiabeteDataset('diabetes_test.csv')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, sampler=ImbalancedDatasetSampler(train_dataset), batch_size=2, num_workers=2)\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ModelDeep(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Input layer is (8 , 8)\n",
    "        Hidder layer is (8, 8)\n",
    "        Ouput layser is (8, 1)\n",
    "        3 layers loss is ~5\n",
    "        8 layers loss is ~4\n",
    "        21 layer losss is ~4\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 8)\n",
    "        self.l2 = torch.nn.Linear(8, 8)\n",
    "        self.l3 = torch.nn.Linear(8, 8)\n",
    "        self.l4 = torch.nn.Linear(8, 8)\n",
    "        self.l5 = torch.nn.Linear(8, 8)\n",
    "        self.l6 = torch.nn.Linear(8, 8)\n",
    "        self.l7 = torch.nn.Linear(8, 8)\n",
    "        self.l8 = torch.nn.Linear(8, 8)\n",
    "        self.l9 = torch.nn.Linear(8, 8)\n",
    "        self.l10 = torch.nn.Linear(8, 8)\n",
    "        self.l11 = torch.nn.Linear(8, 8)\n",
    "        self.l12 = torch.nn.Linear(8, 8)\n",
    "        self.l13 = torch.nn.Linear(8, 8)\n",
    "        self.l14 = torch.nn.Linear(8, 8)\n",
    "        self.l15 = torch.nn.Linear(8, 8)\n",
    "        self.l16 = torch.nn.Linear(8, 8)\n",
    "        self.l17 = torch.nn.Linear(8, 8)\n",
    "        self.l18 = torch.nn.Linear(8, 8)\n",
    "        self.l19 = torch.nn.Linear(8, 8)\n",
    "        self.l20 = torch.nn.Linear(8, 8)\n",
    "        self.l21 = torch.nn.Linear(8, 1)\n",
    "    \n",
    "        # Activation function\n",
    "        self.activation_fn = torch.nn.LeakyReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation_fn(self.l1(x))\n",
    "        x = self.activation_fn(self.l2(x))\n",
    "        x = self.activation_fn(self.l3(x))\n",
    "        x = self.activation_fn(self.l4(x))\n",
    "        x = self.activation_fn(self.l5(x))\n",
    "        x = self.activation_fn(self.l6(x))\n",
    "        x = self.activation_fn(self.l7(x))\n",
    "        x = self.activation_fn(self.l8(x))\n",
    "        x = self.activation_fn(self.l9(x))\n",
    "        x = self.activation_fn(self.l10(x))        \n",
    "        x = self.activation_fn(self.l11(x))\n",
    "        x = self.activation_fn(self.l12(x))        \n",
    "        x = self.activation_fn(self.l13(x))\n",
    "        x = self.activation_fn(self.l14(x))\n",
    "        x = self.activation_fn(self.l15(x))\n",
    "        x = self.activation_fn(self.l16(x))        \n",
    "        x = self.activation_fn(self.l17(x))\n",
    "        x = self.activation_fn(self.l18(x))        \n",
    "        x = self.activation_fn(self.l19(x))\n",
    "        x = self.activation_fn(self.l20(x))        \n",
    "        x = self.sigmoid(self.l21(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWide(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 60)\n",
    "        self.l2 = torch.nn.Linear(60, 60)\n",
    "        self.l3 = torch.nn.Linear(60, 60)\n",
    "        self.l4 = torch.nn.Linear(60, 60)\n",
    "        self.l5 = torch.nn.Linear(60, 60)\n",
    "        self.l6 = torch.nn.Linear(60, 60)\n",
    "        self.l7 = torch.nn.Linear(60, 60)\n",
    "        self.l8 = torch.nn.Linear(60, 60)\n",
    "        self.l9 = torch.nn.Linear(60, 60)\n",
    "        self.l10 = torch.nn.Linear(60, 60)\n",
    "        self.l11 = torch.nn.Linear(60, 60)\n",
    "        self.l12 = torch.nn.Linear(60, 60)\n",
    "        self.l13 = torch.nn.Linear(60, 60)\n",
    "        self.l14 = torch.nn.Linear(60, 1)\n",
    "    \n",
    "        # Activation function\n",
    "        self.activation_fn = torch.nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.activation_fn(self.l1(x))\n",
    "        x = self.activation_fn(self.l2(x))\n",
    "        x = self.activation_fn(self.l3(x))\n",
    "        x = self.activation_fn(self.l4(x))\n",
    "        x = self.activation_fn(self.l5(x))\n",
    "        x = self.activation_fn(self.l6(x))\n",
    "        x = self.activation_fn(self.l7(x))\n",
    "        x = self.activation_fn(self.l8(x))\n",
    "        x = self.activation_fn(self.l9(x))\n",
    "        x = self.activation_fn(self.l10(x))        \n",
    "        x = self.activation_fn(self.l11(x))\n",
    "        x = self.activation_fn(self.l12(x))        \n",
    "        x = self.activation_fn(self.l13(x))\n",
    "        x = self.activation_fn(self.l14(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KopxionAkramsystems(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    loss ~0.6\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 32)\n",
    "        self.l2 = torch.nn.Linear(32, 16)\n",
    "        self.l3 = torch.nn.Linear(16, 1)\n",
    "        self.activation_fn = torch.nn.LeakyReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.activation_fn(self.l1(x))\n",
    "        x = self.activation_fn(self.l2(x))\n",
    "        x = self.sigmoid(self.l3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelDeep()\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/700 (0%)]\tLoss: 0.313262\n",
      "Train Epoch: 0 [20/700 (3%)]\tLoss: 0.813262\n",
      "Train Epoch: 0 [40/700 (6%)]\tLoss: 1.313262\n",
      "Train Epoch: 0 [60/700 (9%)]\tLoss: 0.313262\n",
      "Train Epoch: 0 [80/700 (11%)]\tLoss: 1.313262\n",
      "Train Epoch: 0 [100/700 (14%)]\tLoss: 0.813262\n",
      "Train Epoch: 0 [120/700 (17%)]\tLoss: 0.313262\n",
      "Train Epoch: 0 [140/700 (20%)]\tLoss: 0.313262\n",
      "Train Epoch: 0 [160/700 (23%)]\tLoss: 0.813262\n",
      "Train Epoch: 0 [180/700 (26%)]\tLoss: 0.813262\n",
      "Train Epoch: 0 [200/700 (29%)]\tLoss: 0.313262\n",
      "Train Epoch: 0 [220/700 (31%)]\tLoss: 1.313262\n",
      "Train Epoch: 0 [240/700 (34%)]\tLoss: 0.813262\n",
      "Train Epoch: 0 [260/700 (37%)]\tLoss: 1.313262\n",
      "Train Epoch: 0 [280/700 (40%)]\tLoss: 0.313262\n",
      "Train Epoch: 0 [300/700 (43%)]\tLoss: 1.313262\n",
      "Train Epoch: 0 [320/700 (46%)]\tLoss: 0.313262\n",
      "Train Epoch: 0 [340/700 (49%)]\tLoss: 0.313262\n",
      "Train Epoch: 0 [360/700 (51%)]\tLoss: 0.813262\n",
      "Train Epoch: 0 [380/700 (54%)]\tLoss: 0.813262\n",
      "Train Epoch: 0 [400/700 (57%)]\tLoss: 0.813262\n",
      "Train Epoch: 0 [420/700 (60%)]\tLoss: 0.813262\n",
      "Train Epoch: 0 [440/700 (63%)]\tLoss: 0.813262\n",
      "Train Epoch: 0 [460/700 (66%)]\tLoss: 0.313262\n",
      "Train Epoch: 0 [480/700 (69%)]\tLoss: 1.313262\n",
      "Train Epoch: 0 [500/700 (71%)]\tLoss: 0.313262\n",
      "Train Epoch: 0 [520/700 (74%)]\tLoss: 0.813262\n",
      "Train Epoch: 0 [540/700 (77%)]\tLoss: 0.813262\n",
      "Train Epoch: 0 [560/700 (80%)]\tLoss: 0.313262\n",
      "Train Epoch: 0 [580/700 (83%)]\tLoss: 0.313262\n",
      "Train Epoch: 0 [600/700 (86%)]\tLoss: 0.313262\n",
      "Train Epoch: 0 [620/700 (89%)]\tLoss: 1.313262\n",
      "Train Epoch: 0 [640/700 (91%)]\tLoss: 1.313262\n",
      "Train Epoch: 0 [660/700 (94%)]\tLoss: 1.313262\n",
      "Train Epoch: 0 [680/700 (97%)]\tLoss: 0.813262\n",
      "Train Epoch: 1 [0/700 (0%)]\tLoss: 0.813262\n",
      "Train Epoch: 1 [20/700 (3%)]\tLoss: 0.313262\n",
      "Train Epoch: 1 [40/700 (6%)]\tLoss: 0.813262\n",
      "Train Epoch: 1 [60/700 (9%)]\tLoss: 1.313262\n",
      "Train Epoch: 1 [80/700 (11%)]\tLoss: 0.313262\n",
      "Train Epoch: 1 [100/700 (14%)]\tLoss: 0.313262\n",
      "Train Epoch: 1 [120/700 (17%)]\tLoss: 0.813262\n",
      "Train Epoch: 1 [140/700 (20%)]\tLoss: 1.313262\n",
      "Train Epoch: 1 [160/700 (23%)]\tLoss: 0.813262\n",
      "Train Epoch: 1 [180/700 (26%)]\tLoss: 0.313262\n",
      "Train Epoch: 1 [200/700 (29%)]\tLoss: 0.813262\n",
      "Train Epoch: 1 [220/700 (31%)]\tLoss: 0.313262\n",
      "Train Epoch: 1 [240/700 (34%)]\tLoss: 0.313262\n",
      "Train Epoch: 1 [260/700 (37%)]\tLoss: 1.313262\n",
      "Train Epoch: 1 [280/700 (40%)]\tLoss: 0.813262\n",
      "Train Epoch: 1 [300/700 (43%)]\tLoss: 0.813262\n",
      "Train Epoch: 1 [320/700 (46%)]\tLoss: 0.813262\n",
      "Train Epoch: 1 [340/700 (49%)]\tLoss: 0.313262\n",
      "Train Epoch: 1 [360/700 (51%)]\tLoss: 0.813262\n",
      "Train Epoch: 1 [380/700 (54%)]\tLoss: 0.313262\n",
      "Train Epoch: 1 [400/700 (57%)]\tLoss: 0.313262\n",
      "Train Epoch: 1 [420/700 (60%)]\tLoss: 0.313262\n",
      "Train Epoch: 1 [440/700 (63%)]\tLoss: 1.313262\n",
      "Train Epoch: 1 [460/700 (66%)]\tLoss: 0.813262\n",
      "Train Epoch: 1 [480/700 (69%)]\tLoss: 0.813262\n",
      "Train Epoch: 1 [500/700 (71%)]\tLoss: 0.313262\n",
      "Train Epoch: 1 [520/700 (74%)]\tLoss: 0.813262\n",
      "Train Epoch: 1 [540/700 (77%)]\tLoss: 0.813262\n",
      "Train Epoch: 1 [560/700 (80%)]\tLoss: 0.813262\n",
      "Train Epoch: 1 [580/700 (83%)]\tLoss: 0.813262\n",
      "Train Epoch: 1 [600/700 (86%)]\tLoss: 1.313262\n",
      "Train Epoch: 1 [620/700 (89%)]\tLoss: 0.313262\n",
      "Train Epoch: 1 [640/700 (91%)]\tLoss: 0.813262\n",
      "Train Epoch: 1 [660/700 (94%)]\tLoss: 0.313262\n",
      "Train Epoch: 1 [680/700 (97%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [0/700 (0%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [20/700 (3%)]\tLoss: 1.313262\n",
      "Train Epoch: 2 [40/700 (6%)]\tLoss: 0.313262\n",
      "Train Epoch: 2 [60/700 (9%)]\tLoss: 0.313262\n",
      "Train Epoch: 2 [80/700 (11%)]\tLoss: 0.313262\n",
      "Train Epoch: 2 [100/700 (14%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [120/700 (17%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [140/700 (20%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [160/700 (23%)]\tLoss: 0.313262\n",
      "Train Epoch: 2 [180/700 (26%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [200/700 (29%)]\tLoss: 0.313262\n",
      "Train Epoch: 2 [220/700 (31%)]\tLoss: 1.313262\n",
      "Train Epoch: 2 [240/700 (34%)]\tLoss: 0.313262\n",
      "Train Epoch: 2 [260/700 (37%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [280/700 (40%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [300/700 (43%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [320/700 (46%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [340/700 (49%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [360/700 (51%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [380/700 (54%)]\tLoss: 1.313262\n",
      "Train Epoch: 2 [400/700 (57%)]\tLoss: 0.313262\n",
      "Train Epoch: 2 [420/700 (60%)]\tLoss: 0.313262\n",
      "Train Epoch: 2 [440/700 (63%)]\tLoss: 0.313262\n",
      "Train Epoch: 2 [460/700 (66%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [480/700 (69%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [500/700 (71%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [520/700 (74%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [540/700 (77%)]\tLoss: 0.313262\n",
      "Train Epoch: 2 [560/700 (80%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [580/700 (83%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [600/700 (86%)]\tLoss: 0.313262\n",
      "Train Epoch: 2 [620/700 (89%)]\tLoss: 0.313262\n",
      "Train Epoch: 2 [640/700 (91%)]\tLoss: 1.313262\n",
      "Train Epoch: 2 [660/700 (94%)]\tLoss: 0.313262\n",
      "Train Epoch: 2 [680/700 (97%)]\tLoss: 1.313262\n",
      "Train Epoch: 3 [0/700 (0%)]\tLoss: 0.813262\n",
      "Train Epoch: 3 [20/700 (3%)]\tLoss: 1.313262\n",
      "Train Epoch: 3 [40/700 (6%)]\tLoss: 1.313262\n",
      "Train Epoch: 3 [60/700 (9%)]\tLoss: 1.313262\n",
      "Train Epoch: 3 [80/700 (11%)]\tLoss: 0.813262\n",
      "Train Epoch: 3 [100/700 (14%)]\tLoss: 0.813262\n",
      "Train Epoch: 3 [120/700 (17%)]\tLoss: 0.813262\n",
      "Train Epoch: 3 [140/700 (20%)]\tLoss: 1.313262\n",
      "Train Epoch: 3 [160/700 (23%)]\tLoss: 0.813262\n",
      "Train Epoch: 3 [180/700 (26%)]\tLoss: 1.313262\n",
      "Train Epoch: 3 [200/700 (29%)]\tLoss: 0.813262\n",
      "Train Epoch: 3 [220/700 (31%)]\tLoss: 0.813262\n",
      "Train Epoch: 3 [240/700 (34%)]\tLoss: 1.313262\n",
      "Train Epoch: 3 [260/700 (37%)]\tLoss: 0.313262\n",
      "Train Epoch: 3 [280/700 (40%)]\tLoss: 0.813262\n",
      "Train Epoch: 3 [300/700 (43%)]\tLoss: 0.813262\n",
      "Train Epoch: 3 [320/700 (46%)]\tLoss: 1.313262\n",
      "Train Epoch: 3 [340/700 (49%)]\tLoss: 1.313262\n",
      "Train Epoch: 3 [360/700 (51%)]\tLoss: 0.813262\n",
      "Train Epoch: 3 [380/700 (54%)]\tLoss: 0.313262\n",
      "Train Epoch: 3 [400/700 (57%)]\tLoss: 0.813262\n",
      "Train Epoch: 3 [420/700 (60%)]\tLoss: 0.313262\n",
      "Train Epoch: 3 [440/700 (63%)]\tLoss: 0.313262\n",
      "Train Epoch: 3 [460/700 (66%)]\tLoss: 1.313262\n",
      "Train Epoch: 3 [480/700 (69%)]\tLoss: 1.313262\n",
      "Train Epoch: 3 [500/700 (71%)]\tLoss: 0.813262\n",
      "Train Epoch: 3 [520/700 (74%)]\tLoss: 0.813262\n",
      "Train Epoch: 3 [540/700 (77%)]\tLoss: 0.813262\n",
      "Train Epoch: 3 [560/700 (80%)]\tLoss: 0.313262\n",
      "Train Epoch: 3 [580/700 (83%)]\tLoss: 1.313261\n",
      "Train Epoch: 3 [600/700 (86%)]\tLoss: 0.313262\n",
      "Train Epoch: 3 [620/700 (89%)]\tLoss: 1.313261\n",
      "Train Epoch: 3 [640/700 (91%)]\tLoss: 0.813261\n",
      "Train Epoch: 3 [660/700 (94%)]\tLoss: 0.813261\n",
      "Train Epoch: 3 [680/700 (97%)]\tLoss: 0.313262\n",
      "Train Epoch: 4 [0/700 (0%)]\tLoss: 0.313262\n",
      "Train Epoch: 4 [20/700 (3%)]\tLoss: 0.313262\n",
      "Train Epoch: 4 [40/700 (6%)]\tLoss: 0.813260\n",
      "Train Epoch: 4 [60/700 (9%)]\tLoss: 1.313259\n",
      "Train Epoch: 4 [80/700 (11%)]\tLoss: 0.313264\n",
      "Train Epoch: 4 [100/700 (14%)]\tLoss: 1.313254\n",
      "Train Epoch: 4 [120/700 (17%)]\tLoss: 0.813256\n",
      "Train Epoch: 4 [140/700 (20%)]\tLoss: 0.813248\n",
      "Train Epoch: 4 [160/700 (23%)]\tLoss: 0.813219\n",
      "Train Epoch: 4 [180/700 (26%)]\tLoss: 0.314688\n",
      "Train Epoch: 4 [200/700 (29%)]\tLoss: 0.770353\n",
      "Train Epoch: 4 [220/700 (31%)]\tLoss: 1.060892\n",
      "Train Epoch: 4 [240/700 (34%)]\tLoss: 0.452896\n",
      "Train Epoch: 4 [260/700 (37%)]\tLoss: 0.729404\n",
      "Train Epoch: 4 [280/700 (40%)]\tLoss: 0.987187\n",
      "Train Epoch: 4 [300/700 (43%)]\tLoss: 0.970481\n",
      "Train Epoch: 4 [320/700 (46%)]\tLoss: 0.952193\n",
      "Train Epoch: 4 [340/700 (49%)]\tLoss: 0.714918\n",
      "Train Epoch: 4 [360/700 (51%)]\tLoss: 0.517719\n",
      "Train Epoch: 4 [380/700 (54%)]\tLoss: 0.711838\n",
      "Train Epoch: 4 [400/700 (57%)]\tLoss: 0.711567\n",
      "Train Epoch: 4 [420/700 (60%)]\tLoss: 0.520400\n",
      "Train Epoch: 4 [440/700 (63%)]\tLoss: 0.518532\n",
      "Train Epoch: 4 [460/700 (66%)]\tLoss: 0.524131\n",
      "Train Epoch: 4 [480/700 (69%)]\tLoss: 0.708139\n",
      "Train Epoch: 4 [500/700 (71%)]\tLoss: 0.704700\n",
      "Train Epoch: 4 [520/700 (74%)]\tLoss: 0.702677\n",
      "Train Epoch: 4 [540/700 (77%)]\tLoss: 0.701893\n",
      "Train Epoch: 4 [560/700 (80%)]\tLoss: 0.701895\n",
      "Train Epoch: 4 [580/700 (83%)]\tLoss: 0.828807\n",
      "Train Epoch: 4 [600/700 (86%)]\tLoss: 0.700008\n",
      "Train Epoch: 4 [620/700 (89%)]\tLoss: 0.699245\n",
      "Train Epoch: 4 [640/700 (91%)]\tLoss: 0.592307\n",
      "Train Epoch: 4 [660/700 (94%)]\tLoss: 0.699177\n",
      "Train Epoch: 4 [680/700 (97%)]\tLoss: 0.699366\n",
      "Train Epoch: 5 [0/700 (0%)]\tLoss: 0.810339\n",
      "Train Epoch: 5 [20/700 (3%)]\tLoss: 0.601889\n",
      "Train Epoch: 5 [40/700 (6%)]\tLoss: 0.695951\n",
      "Train Epoch: 5 [60/700 (9%)]\tLoss: 0.631451\n",
      "Train Epoch: 5 [80/700 (11%)]\tLoss: 0.627477\n",
      "Train Epoch: 5 [100/700 (14%)]\tLoss: 0.695990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [120/700 (17%)]\tLoss: 0.695302\n",
      "Train Epoch: 5 [140/700 (20%)]\tLoss: 0.694707\n",
      "Train Epoch: 5 [160/700 (23%)]\tLoss: 0.694302\n",
      "Train Epoch: 5 [180/700 (26%)]\tLoss: 0.694132\n",
      "Train Epoch: 5 [200/700 (29%)]\tLoss: 0.658523\n",
      "Train Epoch: 5 [220/700 (31%)]\tLoss: 0.693568\n",
      "Train Epoch: 5 [240/700 (34%)]\tLoss: 0.693428\n",
      "Train Epoch: 5 [260/700 (37%)]\tLoss: 0.713291\n",
      "Train Epoch: 5 [280/700 (40%)]\tLoss: 0.673592\n",
      "Train Epoch: 5 [300/700 (43%)]\tLoss: 0.693352\n",
      "Train Epoch: 5 [320/700 (46%)]\tLoss: 0.709689\n",
      "Train Epoch: 5 [340/700 (49%)]\tLoss: 0.693222\n",
      "Train Epoch: 5 [360/700 (51%)]\tLoss: 0.693165\n",
      "Train Epoch: 5 [380/700 (54%)]\tLoss: 0.693148\n",
      "Train Epoch: 5 [400/700 (57%)]\tLoss: 0.693140\n",
      "Train Epoch: 5 [420/700 (60%)]\tLoss: 0.691140\n",
      "Train Epoch: 5 [440/700 (63%)]\tLoss: 0.694977\n",
      "Train Epoch: 5 [460/700 (66%)]\tLoss: 0.693143\n",
      "Train Epoch: 5 [480/700 (69%)]\tLoss: 0.691296\n",
      "Train Epoch: 5 [500/700 (71%)]\tLoss: 0.693143\n",
      "Train Epoch: 5 [520/700 (74%)]\tLoss: 0.693147\n",
      "Train Epoch: 5 [540/700 (77%)]\tLoss: 0.696815\n",
      "Train Epoch: 5 [560/700 (80%)]\tLoss: 0.689632\n",
      "Train Epoch: 5 [580/700 (83%)]\tLoss: 0.693176\n",
      "Train Epoch: 5 [600/700 (86%)]\tLoss: 0.693123\n",
      "Train Epoch: 5 [620/700 (89%)]\tLoss: 0.693110\n",
      "Train Epoch: 5 [640/700 (91%)]\tLoss: 0.693151\n",
      "Train Epoch: 5 [660/700 (94%)]\tLoss: 0.693141\n",
      "Train Epoch: 5 [680/700 (97%)]\tLoss: 0.693111\n",
      "Train Epoch: 6 [0/700 (0%)]\tLoss: 0.690667\n",
      "Train Epoch: 6 [20/700 (3%)]\tLoss: 0.689975\n",
      "Train Epoch: 6 [40/700 (6%)]\tLoss: 0.688944\n",
      "Train Epoch: 6 [60/700 (9%)]\tLoss: 0.693160\n",
      "Train Epoch: 6 [80/700 (11%)]\tLoss: 0.693052\n",
      "Train Epoch: 6 [100/700 (14%)]\tLoss: 0.687623\n",
      "Train Epoch: 6 [120/700 (17%)]\tLoss: 0.686810\n",
      "Train Epoch: 6 [140/700 (20%)]\tLoss: 0.693282\n",
      "Train Epoch: 6 [160/700 (23%)]\tLoss: 0.699861\n",
      "Train Epoch: 6 [180/700 (26%)]\tLoss: 0.697840\n",
      "Train Epoch: 6 [200/700 (29%)]\tLoss: 0.693006\n",
      "Train Epoch: 6 [220/700 (31%)]\tLoss: 0.693037\n",
      "Train Epoch: 6 [240/700 (34%)]\tLoss: 0.694418\n",
      "Train Epoch: 6 [260/700 (37%)]\tLoss: 0.693147\n",
      "Train Epoch: 6 [280/700 (40%)]\tLoss: 0.694584\n",
      "Train Epoch: 6 [300/700 (43%)]\tLoss: 0.692958\n",
      "Train Epoch: 6 [320/700 (46%)]\tLoss: 0.691553\n",
      "Train Epoch: 6 [340/700 (49%)]\tLoss: 0.693126\n",
      "Train Epoch: 6 [360/700 (51%)]\tLoss: 0.692754\n",
      "Train Epoch: 6 [380/700 (54%)]\tLoss: 0.693047\n",
      "Train Epoch: 6 [400/700 (57%)]\tLoss: 0.692713\n",
      "Train Epoch: 6 [420/700 (60%)]\tLoss: 0.692458\n",
      "Train Epoch: 6 [440/700 (63%)]\tLoss: 0.692124\n",
      "Train Epoch: 6 [460/700 (66%)]\tLoss: 0.690747\n",
      "Train Epoch: 6 [480/700 (69%)]\tLoss: 0.690973\n",
      "Train Epoch: 6 [500/700 (71%)]\tLoss: 0.693116\n",
      "Train Epoch: 6 [520/700 (74%)]\tLoss: 0.693379\n",
      "Train Epoch: 6 [540/700 (77%)]\tLoss: 0.712108\n",
      "Train Epoch: 6 [560/700 (80%)]\tLoss: 0.672812\n",
      "Train Epoch: 6 [580/700 (83%)]\tLoss: 0.689701\n",
      "Train Epoch: 6 [600/700 (86%)]\tLoss: 0.663913\n",
      "Train Epoch: 6 [620/700 (89%)]\tLoss: 0.657511\n",
      "Train Epoch: 6 [640/700 (91%)]\tLoss: 0.747421\n",
      "Train Epoch: 6 [660/700 (94%)]\tLoss: 0.750069\n",
      "Train Epoch: 6 [680/700 (97%)]\tLoss: 0.639701\n",
      "Train Epoch: 7 [0/700 (0%)]\tLoss: 0.693517\n",
      "Train Epoch: 7 [20/700 (3%)]\tLoss: 0.693937\n",
      "Train Epoch: 7 [40/700 (6%)]\tLoss: 0.693248\n",
      "Train Epoch: 7 [60/700 (9%)]\tLoss: 0.768973\n",
      "Train Epoch: 7 [80/700 (11%)]\tLoss: 0.620044\n",
      "Train Epoch: 7 [100/700 (14%)]\tLoss: 0.602911\n",
      "Train Epoch: 7 [120/700 (17%)]\tLoss: 0.597322\n",
      "Train Epoch: 7 [140/700 (20%)]\tLoss: 0.479699\n",
      "Train Epoch: 7 [160/700 (23%)]\tLoss: 0.790392\n",
      "Train Epoch: 7 [180/700 (26%)]\tLoss: 0.832134\n",
      "Train Epoch: 7 [200/700 (29%)]\tLoss: 0.435052\n",
      "Train Epoch: 7 [220/700 (31%)]\tLoss: 0.476361\n",
      "Train Epoch: 7 [240/700 (34%)]\tLoss: 0.569372\n",
      "Train Epoch: 7 [260/700 (37%)]\tLoss: 0.565421\n",
      "Train Epoch: 7 [280/700 (40%)]\tLoss: 0.319111\n",
      "Train Epoch: 7 [300/700 (43%)]\tLoss: 0.503203\n",
      "Train Epoch: 7 [320/700 (46%)]\tLoss: 0.503204\n",
      "Train Epoch: 7 [340/700 (49%)]\tLoss: 0.693123\n",
      "Train Epoch: 7 [360/700 (51%)]\tLoss: 0.813262\n",
      "Train Epoch: 7 [380/700 (54%)]\tLoss: 0.695610\n",
      "Train Epoch: 7 [400/700 (57%)]\tLoss: 1.311689\n",
      "Train Epoch: 7 [420/700 (60%)]\tLoss: 0.503204\n",
      "Train Epoch: 7 [440/700 (63%)]\tLoss: 0.313262\n",
      "Train Epoch: 7 [460/700 (66%)]\tLoss: 0.503204\n",
      "Train Epoch: 7 [480/700 (69%)]\tLoss: 0.313272\n",
      "Train Epoch: 7 [500/700 (71%)]\tLoss: 0.500217\n",
      "Train Epoch: 7 [520/700 (74%)]\tLoss: 0.503285\n",
      "Train Epoch: 7 [540/700 (77%)]\tLoss: 0.503204\n",
      "Train Epoch: 7 [560/700 (80%)]\tLoss: 0.813262\n",
      "Train Epoch: 7 [580/700 (83%)]\tLoss: 0.504353\n",
      "Train Epoch: 7 [600/700 (86%)]\tLoss: 0.503204\n",
      "Train Epoch: 7 [620/700 (89%)]\tLoss: 0.503204\n",
      "Train Epoch: 7 [640/700 (91%)]\tLoss: 0.693147\n",
      "Train Epoch: 7 [660/700 (94%)]\tLoss: 0.503198\n",
      "Train Epoch: 7 [680/700 (97%)]\tLoss: 0.693147\n",
      "Train Epoch: 8 [0/700 (0%)]\tLoss: 0.693178\n",
      "Train Epoch: 8 [20/700 (3%)]\tLoss: 1.003171\n",
      "Train Epoch: 8 [40/700 (6%)]\tLoss: 0.321155\n",
      "Train Epoch: 8 [60/700 (9%)]\tLoss: 0.693147\n",
      "Train Epoch: 8 [80/700 (11%)]\tLoss: 0.999760\n",
      "Train Epoch: 8 [100/700 (14%)]\tLoss: 0.503704\n",
      "Train Epoch: 8 [120/700 (17%)]\tLoss: 0.503204\n",
      "Train Epoch: 8 [140/700 (20%)]\tLoss: 0.313262\n",
      "Train Epoch: 8 [160/700 (23%)]\tLoss: 0.503205\n",
      "Train Epoch: 8 [180/700 (26%)]\tLoss: 0.504893\n",
      "Train Epoch: 8 [200/700 (29%)]\tLoss: 0.503199\n",
      "Train Epoch: 8 [220/700 (31%)]\tLoss: 0.504597\n",
      "Train Epoch: 8 [240/700 (34%)]\tLoss: 0.319411\n",
      "Train Epoch: 8 [260/700 (37%)]\tLoss: 0.503211\n",
      "Train Epoch: 8 [280/700 (40%)]\tLoss: 0.693147\n",
      "Train Epoch: 8 [300/700 (43%)]\tLoss: 0.507231\n",
      "Train Epoch: 8 [320/700 (46%)]\tLoss: 0.693147\n",
      "Train Epoch: 8 [340/700 (49%)]\tLoss: 0.313989\n",
      "Train Epoch: 8 [360/700 (51%)]\tLoss: 0.313448\n",
      "Train Epoch: 8 [380/700 (54%)]\tLoss: 0.504222\n",
      "Train Epoch: 8 [400/700 (57%)]\tLoss: 0.313999\n",
      "Train Epoch: 8 [420/700 (60%)]\tLoss: 0.503316\n",
      "Train Epoch: 8 [440/700 (63%)]\tLoss: 0.770770\n",
      "Train Epoch: 8 [460/700 (66%)]\tLoss: 0.813246\n",
      "Train Epoch: 8 [480/700 (69%)]\tLoss: 0.504124\n",
      "Train Epoch: 8 [500/700 (71%)]\tLoss: 0.313277\n",
      "Train Epoch: 8 [520/700 (74%)]\tLoss: 0.503251\n",
      "Train Epoch: 8 [540/700 (77%)]\tLoss: 0.503204\n",
      "Train Epoch: 8 [560/700 (80%)]\tLoss: 0.693142\n",
      "Train Epoch: 8 [580/700 (83%)]\tLoss: 0.693147\n",
      "Train Epoch: 8 [600/700 (86%)]\tLoss: 0.313262\n",
      "Train Epoch: 8 [620/700 (89%)]\tLoss: 0.693147\n",
      "Train Epoch: 8 [640/700 (91%)]\tLoss: 0.503326\n",
      "Train Epoch: 8 [660/700 (94%)]\tLoss: 0.503204\n",
      "Train Epoch: 8 [680/700 (97%)]\tLoss: 0.693147\n",
      "Train Epoch: 9 [0/700 (0%)]\tLoss: 0.807266\n",
      "Train Epoch: 9 [20/700 (3%)]\tLoss: 0.503223\n",
      "Train Epoch: 9 [40/700 (6%)]\tLoss: 0.503890\n",
      "Train Epoch: 9 [60/700 (9%)]\tLoss: 0.313262\n",
      "Train Epoch: 9 [80/700 (11%)]\tLoss: 0.313350\n",
      "Train Epoch: 9 [100/700 (14%)]\tLoss: 0.503204\n",
      "Train Epoch: 9 [120/700 (17%)]\tLoss: 1.001704\n",
      "Train Epoch: 9 [140/700 (20%)]\tLoss: 0.503204\n",
      "Train Epoch: 9 [160/700 (23%)]\tLoss: 0.693147\n",
      "Train Epoch: 9 [180/700 (26%)]\tLoss: 1.003204\n",
      "Train Epoch: 9 [200/700 (29%)]\tLoss: 0.948525\n",
      "Train Epoch: 9 [220/700 (31%)]\tLoss: 0.505211\n",
      "Train Epoch: 9 [240/700 (34%)]\tLoss: 0.503204\n",
      "Train Epoch: 9 [260/700 (37%)]\tLoss: 0.506566\n",
      "Train Epoch: 9 [280/700 (40%)]\tLoss: 0.693147\n",
      "Train Epoch: 9 [300/700 (43%)]\tLoss: 0.693147\n",
      "Train Epoch: 9 [320/700 (46%)]\tLoss: 0.693147\n",
      "Train Epoch: 9 [340/700 (49%)]\tLoss: 0.693147\n",
      "Train Epoch: 9 [360/700 (51%)]\tLoss: 0.503204\n",
      "Train Epoch: 9 [380/700 (54%)]\tLoss: 0.693147\n",
      "Train Epoch: 9 [400/700 (57%)]\tLoss: 0.693147\n",
      "Train Epoch: 9 [420/700 (60%)]\tLoss: 0.324386\n",
      "Train Epoch: 9 [440/700 (63%)]\tLoss: 0.693147\n",
      "Train Epoch: 9 [460/700 (66%)]\tLoss: 0.503210\n",
      "Train Epoch: 9 [480/700 (69%)]\tLoss: 0.503205\n",
      "Train Epoch: 9 [500/700 (71%)]\tLoss: 0.693156\n",
      "Train Epoch: 9 [520/700 (74%)]\tLoss: 0.503205\n",
      "Train Epoch: 9 [540/700 (77%)]\tLoss: 0.503339\n",
      "Train Epoch: 9 [560/700 (80%)]\tLoss: 0.693147\n",
      "Train Epoch: 9 [580/700 (83%)]\tLoss: 0.503204\n",
      "Train Epoch: 9 [600/700 (86%)]\tLoss: 0.503206\n",
      "Train Epoch: 9 [620/700 (89%)]\tLoss: 0.813262\n",
      "Train Epoch: 9 [640/700 (91%)]\tLoss: 0.813208\n",
      "Train Epoch: 9 [660/700 (94%)]\tLoss: 1.003167\n",
      "Train Epoch: 9 [680/700 (97%)]\tLoss: 0.503205\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        resized_target = target.view(output.shape[0], -1)\n",
    "        loss = criterion(output, resized_target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "\n",
    "classes = (True, False)\n",
    "\n",
    "def boolean_output(output):\n",
    "    return bool(output.view(1) > torch.tensor([0.5]))\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    print(cm)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            bool_output = boolean_output(output)\n",
    "            bool_target = bool(target)\n",
    "            y_test.append(bool_target)\n",
    "            y_pred.append(bool_output)\n",
    "            if bool_output == bool_target:\n",
    "                correct += 1\n",
    "    accuracy = correct/len(test_loader.dataset)\n",
    "    print(f\"accuracy: {accuracy}\")\n",
    "    # Confusion matrix\n",
    "    confusion_mtx = confusion_matrix(y_test, y_pred)\n",
    "    plot_confusion_matrix(confusion_mtx, classes=classes, normalize=True,\n",
    "                          title='Confusion matrix')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7647058823529411\n",
      "[[36  5]\n",
      " [11 16]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test(model, \"cpu\", test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
